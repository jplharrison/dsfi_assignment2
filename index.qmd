---
title: "DSFI_A2"
---

This is a Quarto website.

To learn more about Quarto websites visit <https://quarto.org/docs/websites>.



```{python}
pip install transformers
```

```{python}
#| include: false
# General imports
import pickle
from joblib import dump, load
import os
import pandas as pd
import re
import numpy as np
import string

# NLTK imports
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Visualization imports
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS

# Preprocessing imports
from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS, TfidfVectorizer
from sklearn.preprocessing import LabelEncoder

# Model selection imports
from sklearn.model_selection import train_test_split, GridSearchCV

# Machine learning model imports
from sklearn.svm import SVC
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from catboost import Pool, cv, CatBoostClassifier

# Word embedding imports
from gensim.models import Word2Vec

# Metrics import
from sklearn.metrics import classification_report, accuracy_score
```


```{python}
#| echo: false
#| eval: false
#| include: false
folder_path = 'speeches'  
files = os.listdir(folder_path)
files = sorted([file for file in files if os.path.isfile(os.path.join(folder_path, file)) and file.endswith('.txt')])

president_names = []

pattern = r'_(.+?)\.txt'  
for file in files:
    match = re.search(pattern, file)
    if match:
        president_name = match.group(1)
        # Remove the "_2" suffix from the president names here
        cleaned_president_name = president_name.replace('_2', '')
        president_names.append(cleaned_president_name)
    else:
        print(f"Warning: No match found in filename: {file}")
        president_names.append('Unknown')  # Placeholder for missing names

# Check  lengths
if len(files) != len(president_names):
    print(f"Warning: Number of files ({len(files)}) does not match number of president names ({len(president_names)})")

def preprocess_speeches(speech):
    # Tokenize the text
    tokens = word_tokenize(speech)
    # Remove punctuation and convert to lowercase
    tokens = [token.lower() for token in tokens if token not in string.punctuation]
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [token for token in tokens if token not in stop_words]
    # Lemmatize tokens
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(token) for token in tokens]
    return tokens


df = pd.DataFrame(columns=['Presidents', 'Sentences'])
tokenised_speeches = pd.DataFrame(columns=['Presidents', 'Tokens'])  # To store the speeches tokenized by word

# Iterate over all files and extract sentence
for file_index in range(len(files)):
    file_path = os.path.join(folder_path, files[file_index])
    with open(file_path, 'r', encoding='utf-8') as file:
        speech = file.read()    
        tokens = preprocess_speeches(speech)

        lines = file.readlines()[2:] 

    text = ' '.join(lines)
    sentences = sent_tokenize(text)
    cleaned_sentences = [sentence.replace('\n', '') for sentence in sentences]

    current_president = president_names[file_index]
    dftemp = pd.DataFrame({'Presidents': [current_president] * len(cleaned_sentences), 'Sentences': cleaned_sentences})
    dftemp2 = pd.DataFrame({'Presidents': [current_president] * len(tokens), 'Tokens': tokens})
    df = pd.concat([df, dftemp], axis=0, ignore_index=True)
    tokenised_speeches = pd.concat([tokenised_speeches, dftemp2], axis=0, ignore_index=True)

df.reset_index(drop=True, inplace=True)
tokenised_speeches.reset_index(drop=True, inplace=True)
tokenised_speeches = tokenised_speeches[~tokenised_speeches['Tokens'].str.contains(r'[0-9]', na=False)]  ## Remove numeric tokens


# Save the DataFrame to a CSV file
#df.to_csv('finalSentences2.csv', index=False)
#tokenised_speeches.to_csv('finalTokens.csv', index=False)

```


```{python}
a2data = pd.read_csv("finalSentences2.csv")
a2tokens = pd.read_csv("finalTokens.csv")
```

# Sentiment Analysis

## Hugging Face 1 
link: https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest?text=ayo
```{python}
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig
import numpy as np
from scipy.special import softmax


```


```{python}
#| echo: false
#| eval: false
#| include: false
data = a2data
# Define the preprocess function
def preprocess(text):
    new_text = []
    for t in text.split(" "):
        t = '@user' if t.startswith('@') and len(t) > 1 else t
        t = 'http' if t.startswith('http') else t
        new_text.append(t)
    return " ".join(new_text)

# Define the model details
MODEL = "cardiffnlp/twitter-roberta-base-sentiment-latest"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
config = AutoConfig.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)

# Preprocess the sentences
data['Preprocessed_Sentences'] = data['Sentences'].apply(preprocess)

# Predict sentiments for the preprocessed sentences
def predict_sentiment(text):
    encoded_input = tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
    output = model(**encoded_input)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    ranking = np.argsort(scores)
    ranking = ranking[::-1]
    label = config.id2label[ranking[0]]
    score = scores[ranking[0]]
    return label, np.round(float(score), 4)

sentiments = data['Preprocessed_Sentences'].apply(predict_sentiment)
data['Predicted_Label'] = [s[0] for s in sentiments]
data['Score'] = [s[1] for s in sentiments]

data.head()
data.to_csv("HFM1.csv")
```


```{python}
HFM1 = pd.read_csv("HFM1.csv")
mean_scores_reordered = mean_scores_per_president.reindex(adjusted_presidents_order)

# Plotting the reordered mean sentiment scores
plt.figure(figsize=(12, 7))
mean_scores_reordered.plot(kind='bar', color='skyblue')
plt.title('Mean Sentiment Scores per President')
plt.xlabel('President')
plt.ylabel('Mean Sentiment Score')
plt.axhline(0, color='red', linestyle='--')  # Add a line at y=0 for reference
plt.tight_layout()
plt.show()
```


```{python}
colors = {'Negative': '#FF9999', 'Neutral': '#99CCFF', 'Positive': '#99FF99'}

sentiment_counts.reindex(adjusted_presidents_order).plot(kind='bar', stacked=True, figsize=(12,7), color=[colors[col] for col in sentiment_counts.columns])
plt.title('Sentiment Distribution per President')
plt.ylabel('Percentage')
plt.xlabel('President')
plt.legend(title='Sentiment')
plt.tight_layout()
plt.show()
```